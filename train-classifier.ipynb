{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "\n",
    "We start by cleaning the tweets we will be using to train our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"'\", '', text)\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'pic.twitter\\S+', '', text)\n",
    "    text = re.sub(r'\\W+', ' ', text.lower())\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "df = pd.read_csv(os.path.join('tweets', 'tweets.csv'),\n",
    "                 low_memory=False)\n",
    "df.drop_duplicates(inplace=True)\n",
    "df['tweet-clean'] = df['tweet'].apply(clean_text)\n",
    "drop_index = []\n",
    "\n",
    "for i in range(len(df)):\n",
    "    if df['tweet-clean'].iloc[i] in ('', ' '):\n",
    "        drop_index.append(i)\n",
    "\n",
    "df.drop(drop_index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will set two constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 0\n",
    "n_jobs = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that by setting ```random_state = 0```, we are not actually initializing our random seed, but will be passing an integer to our various scikit-learn implementations which will allow us to keep the same random state with each run of our code. While tuning hyperparameters, we want to ensure that we generate the same random sequence each time we test our model so that we can be sure that any improvement in performance that we see is due to the changes we made and not due to randomness created by the random number generator.\n",
    "\n",
    "Scikit-learn supports multithreading. Most computers today use a multi-core CPU. We can set ```n_jobs = -1``` to use all the threads on a CPU for faster model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will compute the tfidf vectors for our tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 34648\n",
      "Size of vocabulary:  86092\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, 2), min_df=2)\n",
    "X = tfidf.fit_transform(df['tweet-clean'])\n",
    "\n",
    "print(f'Number of documents: {X.shape[0]}')\n",
    "print(f'Size of vocabulary:  {X.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To stay consistent throughout the notebook, we’ll assign them as:\n",
    "\n",
    "Number of documents: n = 34 648 tweets\n",
    "\n",
    "Size of the vocabulary: v = 86 092 terms\n",
    "\n",
    "We set ```ngram_range=(1, 2)``` and ```min_df=2``` which states that our vocabulary will consist of unigrams and bigrams with a document frequency of at least 2. By eliminating terms with a document frequency of only 1, we can reduce the number of features in our model and reduce overfitting. This method works well here because we have tens of thousands of tweets. This may not be appropriate for projects containing fewer documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will encode our labels as integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernie Sanders  = 0\n",
      "Donald J. Trump = 1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(df['name'])\n",
    "\n",
    "for i in range(len(le.classes_)):\n",
    "    print(f'{le.classes_[i]:<15} = {i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training our model, it is important that we split our data set into a train and test set. The training set will be used to train our model, but it is important to have a separate testing data set to get a true measure of how accurate our model is on data that it has never seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                    test_size=0.5,\n",
    "                                    random_state=random_state,\n",
    "                                    stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we specified that we will keep 50% of the data set as our testing set with ```test_size=0.5```. We also set ```stratify=y``` to ensure that our test and training data set have the same ratio of Bernie Sanders to Donald Trump tweets."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Logistic regression\n",
    "\n",
    "The standard logistic function:\n",
    "\n",
    "$$\\phi\\left(z\\right)=\\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "receives a value *z* and outputs a value between 0 and 1. We are working with a binary classification problem, meaning that the result must be Bernie Sanders or Donald Trump, with no other options. This is why we previously assigned each politician a value of 0 or 1. Now let’s take a look at how we can convert a tweet into a value *z*:\n",
    "\n",
    "$$z=b_0+w_1x_1+w_2x_2+\\ldots+w_vx_v$$\n",
    "\n",
    "From this equation, we see that *z* is equal to the dot product of our tfidf vector (**x**) with the weight vector (**w**) to which a bias term (*$b_0$*) is added. Note that although Python lists begin indexing at 0, it is more common to begin indexing at 1 when describing the mathematical model and reserving the index 0 for our bias term (which can sometimes also appear as *$w_0$*). For each tweet, we can calculate a value *z*, pass it through the logistic function, and round to the nearest integer to get 0=Bernie Sanders or 1=Donald Trump. To do that, our machine learning algorithm needs to figure out which values for the weight vector (**w**) will result in the highest percentage of tweets being attributed to the correct politician.\n",
    "\n",
    "We won’t look into the various methods of solving for **w** here. Scikit-learn implements various solvers for training a logistic regression classifier. In this app we set ```solver='saga'``` as our optimization method and ```C=20``` as our inverse regularization term to reduce overfitting and make it easier to train our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression accuracy: 95.8%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf_log = LogisticRegression(C=20, solver='saga',\n",
    "                             random_state=random_state,\n",
    "                             n_jobs=n_jobs)\n",
    "\n",
    "clf_log.fit(X_train, y_train)\n",
    "log_score = clf_log.score(X_test, y_test)\n",
    "print(f'Logistic Regression accuracy: {log_score:.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bernoulli naive Bayes\n",
    "\n",
    "Naive Bayes classifiers are relatively popular for text classification tasks. We will implement a specific event model known as Bernoulli naive Bayes. The likelihood of a specific tweet having been written by each politician can be calculated as follows:\n",
    "\n",
    "$$p\\left(\\mathbf{x}\\ \\right|\\ C_k)=\\ \\prod_{i=1}^{v}{p_{ki}^{x_i}\\left(1-p_{ki}\\right)^{(1-x_i)}}$$\n",
    "\n",
    "where *$p_{ki}$*, is the probability that politician *$C_k$*, will use the term *i* in a tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB(alpha=0.01, binarize=0.09, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "clf_bnb = BernoulliNB(alpha=0.01, binarize=0.09)\n",
    "clf_bnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s consider the following Bernie Sanders quote:\n",
    "\n",
    "*“Medicare for all”* - Bernie Sanders\n",
    "\n",
    "This quote will contain the term “medicare for”. Let’s see how this translates into the parameters in the above equation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 0\n",
      "i = 44798\n",
      "C_k = Bernie Sanders\n",
      "p_ki = 0.0289\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "C_k = 'Bernie Sanders'\n",
    "k = le.transform([C_k])[0]\n",
    "i = tfidf.vocabulary_['medicare for']\n",
    "p_ki = np.exp(clf_bnb.feature_log_prob_[k, i])\n",
    "print(f'k = {k}')\n",
    "print(f'i = {i}')\n",
    "print(f'C_k = {C_k}')\n",
    "print(f'p_ki = {p_ki:.3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, *$p_{ki}$* = 0.0289 can be interpreted as 2.89% of tweets from Bernie Sanders in our training set contain the term “medicare for”. What’s interesting about Bernoulli naive Bayes is that the *$p_{ki}$* values can be solved for directly. It is simply equal to the document frequency of term *i* in politician *$C_k$*’s corpus divided by the total number of documents in politician *$C_k$*’s corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.028924\n",
      "0.028922\n"
     ]
    }
   ],
   "source": [
    "df_ki = clf_bnb.feature_count_[k, i]\n",
    "n_k = clf_bnb.class_count_[k]\n",
    "p_ki_manual = df_ki / n_k\n",
    "print(f'{p_ki:.5}')\n",
    "print(f'{p_ki_manual:.5}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In logistic regression, **x** contained tfidf values. In Bernoulli naive Bayes, *$x_i$* must equal to 0 or 1. In its simplest form, *$x_i$* would equal to 1 if the term *i* was present in the tweet, and 0 if it was absent. However, we can extract a bit more information by setting a threshold on our tfidf values instead. An optimal threshold is typically found through trial and error or exhaustive search methods such as GridSearchCV in scikit-learn. For this app, an optimal threshold was ```binarize=0.09```. Therefore, any tfidf value above 0.09 was set to 1, and any below was set to 0. We also set ```alpha=0.01``` which is a smoothing parameter.\n",
    "\n",
    "Going back to our equation, we can see that if we receive a document from the user whereby *$x_i$* = 1, then we are multiplying the probability (*$p_{ki}$*) of the term *i* appearing in politician *$C_k$*’s tweet. Conversely, if *$x_i$* = 0, then we are multiplying the probability that the term *i* would not appear in politician *$C_k$*’s tweet. This multiplication is done for each term in the vocabulary and for each politician. Then the politician with the highest joint log-likelihood is output as the result. Remember that while the logistic function outputs a probability, Bernoulli naive Bayes outputs a likelihood. These two terms are related, but do not mean the same thing. However, ```BernoulliNB``` can convert the likelihood into a probability using its ```predict_proba``` method. Now let’s calculate our accuracy on the test data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Naive Bayes accuracy: 96.2%\n"
     ]
    }
   ],
   "source": [
    "bnb_score = clf_bnb.score(X_test, y_test)\n",
    "print(f'Bernoulli Naive Bayes accuracy: {bnb_score:.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble averaging\n",
    "\n",
    "We trained two models, logistic regression and Bernoulli naive Bayes, both with a relatively high accuracy on the test data set. Using scikit-learn’s ```VotingClassifier```, we can take the weighted average of the probabilities they each calculate by selecting our classifiers with ```estimators=[('log', clf_log), ('bnb', clf_bnb)]``` and setting ```voting='soft'``` to obtain a more accurate result. Adding 60% of the logistic regression probability with 40% of the Bernoulli naive Bayes probability by setting ```weights=(0.6, 0.4)``` had the highest accuracy. An interesting result, considering that our Bernoulli naive Bayes classifier had a higher accuracy, but we are assigning it less weight. This is likely because the underlying estimation of the probabilities from our Bernoulli naive Bayes classifier is not necessarily as reliable as the probability calculated from our logistic regression classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Averaging accuracy: 96.4%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "clf_vot = VotingClassifier(\n",
    "    estimators=[('log', clf_log), ('bnb', clf_bnb)],\n",
    "    voting='soft', weights=(0.6, 0.4), n_jobs=n_jobs)\n",
    "\n",
    "clf_vot.fit(X_train, y_train)\n",
    "vot_score = clf_vot.score(X_test, y_test)\n",
    "print(f'Ensemble Averaging accuracy: {vot_score:.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this is a marginal increase in accuracy, we should keep in mind that people will likely input messages that are not necessarily something that either candidate might say. In these cases, ensemble averaging can be beneficial since it’s the equivalent to getting a second opinion before making a decision on which politician was more likely to tweet what the user had written."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final training\n",
    "\n",
    "Once we are sure that we are not going to change any more hyperparameters (*e.g.*, the threshold on naive Bayes, the weighted average for voting, *etc.*) then we can retrain our model on the entire data set to get twice as many training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('log',\n",
       "                              LogisticRegression(C=20, class_weight=None,\n",
       "                                                 dual=False, fit_intercept=True,\n",
       "                                                 intercept_scaling=1,\n",
       "                                                 l1_ratio=None, max_iter=100,\n",
       "                                                 multi_class='auto', n_jobs=-1,\n",
       "                                                 penalty='l2', random_state=0,\n",
       "                                                 solver='saga', tol=0.0001,\n",
       "                                                 verbose=0, warm_start=False)),\n",
       "                             ('bnb',\n",
       "                              BernoulliNB(alpha=0.01, binarize=0.09,\n",
       "                                          class_prior=None, fit_prior=True))],\n",
       "                 flatten_transform=True, n_jobs=-1, voting='soft',\n",
       "                 weights=(0.6, 0.4))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_vot.fit(X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:who-said-it] *",
   "language": "python",
   "name": "conda-env-who-said-it-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
